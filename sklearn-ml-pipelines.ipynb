{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning using scikit-learn\n",
    "> Best practices made simple with pipelines\n",
    "\n",
    "In this notebook, we demonstrate the usage of pipelines to promote best practices in ML in python.  We'll make sure that all of our pre-processing steps are included within the cross validation training loop, measure performance using cross validation, and make sure to use literate programming practices to share our work.\n",
    "\n",
    "# Technical objective\n",
    "Our technical objective here is to predict the sex of a penguin given its other distinguishing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tables and visualizations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "#machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, StandardScaler\n",
    "from sklearn import config_context\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "In this example, we'll use Allison Horst's penguins dataset.  Although this is normally used in R, we grab the csv from the penguins [GitHub repo](https://github.com/allisonhorst/palmerpenguins).  More information on the dataset is available on the introduction page [here.](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and learn a bit\n",
    "peng_data = pd.read_csv('https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv')\n",
    "display(peng_data.head())\n",
    "peng_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and EDA\n",
    "We can now explore our data.  We leave this exercise to the reader.  For now, we can observe that there are a few NA values which will likely need imputation.  We'll wait for this step so that we can put it within our training loop.  For now, we'll just drop all of the sex NAs out of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_data = peng_data.dropna(subset=['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peng_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of dimensions is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data\n",
    "Here, we employ the initial split to separate the training from the golden holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_column = 'sex'\n",
    "random_seed = 2435\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(peng_data.drop(columns=class_column), peng_data[class_column],\n",
    "                                                   test_size=0.25, random_state=random_seed, stratify=peng_data[class_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check to make sure that everything seems OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Train\n",
    "print('On X train: ')\n",
    "print('X train dimensions: ', X_train.shape)\n",
    "display(X_train.head())\n",
    "\n",
    "# X test\n",
    "print('\\nOn X test: ')\n",
    "print('X test dimensions: ', X_test.shape)\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Train\n",
    "print('On y train: ')\n",
    "print('y train dimensions: ', y_train.shape)\n",
    "display(y_train.head())\n",
    "\n",
    "# X test\n",
    "print('\\nOn y test: ')\n",
    "print('y test dimensions: ', y_test.shape)\n",
    "display(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I love a good sanity check.  Here, we can see that the ratios of the split are about correct.  We can also observe that the indices match (for example, between X and y train).  These are all good indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish training pipeline\n",
    "It looks like we first might need to do some imputation on some values within our rows.  Then, it looks like we have some categorical columns so those will need to be encoded.  However, this isn't all the columns since our column types are heterogeneous.  Let's see what transforming specific columns looks like here.\n",
    "\n",
    "Note that in the end, this will be methodology-specific.  Although we use logistic regression below, we also perform imputation to demonstrate the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#individual pipelines for differing datatypes\n",
    "cat_pipeline = Pipeline(steps=[('cat_impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "                               ('onehot_cat', OneHotEncoder(drop='if_binary'))])\n",
    "num_pipeline = Pipeline(steps=[('impute_num', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "                               ('scale_num', StandardScaler())])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish preprocessing pipeline by columns\n",
    "preproc = ColumnTransformer([('cat_pipe', cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "                             ('num_pipe', num_pipeline, make_column_selector(dtype_include=np.number))],\n",
    "                             remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the whole modeling pipeline with preprocessing\n",
    "pipe = Pipeline(steps=[('preproc', preproc),\n",
    "                       ('mdl', LogisticRegression(penalty='elasticnet', solver='saga', tol=0.01))])\n",
    "\n",
    "#visualization for steps\n",
    "with config_context(display='diagram'):\n",
    "    display(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_grid = {'mdl__l1_ratio' : np.linspace(0,1,5),\n",
    "               'mdl__C': np.logspace(-1, 6, 3) }\n",
    "grid_search = GridSearchCV(pipe, param_grid = tuning_grid, cv = 5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_score_)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final fit\n",
    "The final fit here is already present in the generated model due to the way we set our parameters in the grid search.  If we want to look at the performance, we can do so.  Here is a non-helpful description of the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable importance\n",
    "Now we assess the importance in the selected model to reveal any potential insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip = grid_search.best_estimator_['mdl'].coef_[0]\n",
    "vip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get names in correct preproc order\n",
    "cat_names = grid_search.best_estimator_.named_steps['preproc'].transformers_[0][1].named_steps['onehot_cat'].get_feature_names()\n",
    "num_names = grid_search.best_estimator_.named_steps['preproc'].transformers_[1][2]\n",
    "\n",
    "#create df with vip info\n",
    "coef_info = pd.DataFrame({'feat_names':np.hstack([cat_names, num_names]), 'vip': vip})\n",
    "\n",
    "#get sign and magnitude information\n",
    "coef_info = coef_info.assign(coef_mag = abs(coef_info['vip']),\n",
    "                             coef_sign = np.sign(coef_info['vip']))\n",
    "\n",
    "#sort and plot\n",
    "coef_info = coef_info.set_index('feat_names').sort_values(by='coef_mag', ascending=False)\n",
    "sns.barplot(y=coef_info.index, x='coef_mag', hue='coef_sign', data=coef_info, orient='h', dodge=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the performance of the model, which is pretty nice!  We can also look into different scores specifically for more insight into the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
